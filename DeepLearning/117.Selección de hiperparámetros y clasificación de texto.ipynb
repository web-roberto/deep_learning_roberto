{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Tuner: Reuters Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es un conjunto de datos de 11.228 noticias de Reuters, etiquetadas en 46 temas.\n",
    "\n",
    "Esto fue originalmente generado por el análisis y preprocesamiento del clásico conjunto de datos de Reuters-21578, pero el código de preprocesamiento ya no está empaquetado con Keras. \n",
    "\n",
    "Cada noticia está codificado como una lista de índices de palabras (números enteros). Para mayor comodidad, las palabras se indexan por la frecuencia general en el conjunto de datos, de modo que, por ejemplo, el número entero \"3\" codifica la tercera palabra más frecuente en los datos. Esto permite realizar operaciones de filtrado rápido como: \"considerar sólo las 10.000 palabras más comunes, pero eliminar las 20 palabras más comunes\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargando el conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento del conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pesar de que ya tenemos codificadas las reseñas para que esten representadas por valores numéricos en lugar de por cadenas de texto, existen algunos factores que no permiten que sean conjuntos de datos apropiados para proporcionarle a una red neuronal artifical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def vectorize(seqs, dim=10000):\n",
    "    results = np.zeros((len(seqs), dim))\n",
    "    for i, seq in enumerate(seqs):\n",
    "        results[i, seq] = 1.\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizamos tanto el conjunto de datos de entrenamiento, como el conjunto de datos de pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizamos el conjunto de datos de entrenamiento y el de pruebas\n",
    "X_train = vectorize(train_data, 10000)\n",
    "X_test = vectorize(test_data, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cada una de las reseñas se corresponde con un vector formado por 1s y 0s\n",
    "print(\"Valores originales:\\t\", train_data[0][:15])\n",
    "print(\"Valores vectorizados:\\t\", X_train[0][:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformamos las etiquetas de salida en vectores\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "Y_train = to_categorical(train_labels)\n",
    "Y_test = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## División del conjunto de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_test, X_val, Y_test, Y_val = train_test_split(X_test, Y_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Longitud subconjunto de entrenamiento: \", len(X_train))\n",
    "print(\"Longitud subconjunto de validación: \", len(X_val))\n",
    "print(\"Longitud subconjunto de pruebas: \", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción de la red neuronal artificial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para la realización del tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "import kerastuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    # Definición del modelo\n",
    "    model = keras.Sequential()\n",
    "    # Tuning del número de neuronas de TODAS las hidden layer\n",
    "    hp_units = hp.Int('units', min_value=8, max_value=128, step=8)\n",
    "    # Tuning del número de capas para cada capa hidden\n",
    "    hp_layers = hp.Int('layers', min_value=1, max_value=5, step=1)\n",
    "    # Input layer\n",
    "    model.add(layers.Dense(units=128, activation='relu', input_shape=(10000,)))\n",
    "    # Hidden layers \n",
    "    for i in range(hp_layers):\n",
    "        # Añadimos una hidden layer en cada iteración del bucle\n",
    "        model.add(layers.Dense(units=hp_units, activation='relu'))\n",
    "    # Output layer -> 46 temas posibles en las noticias\n",
    "    model.add(layers.Dense(46, activation='softmax'))\n",
    "    # Seleccionamos el valor optimo entre [0.01, 0.001, 0.0001]\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate), \n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    model_builder,\n",
    "    objective= 'val_accuracy',\n",
    "    max_epochs=10,\n",
    "    factor=3, # nº de entrenamientos por ronda del Hyper\n",
    "    directory='hp_dir',\n",
    "    project_name='ruters_dataset'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una función de callback que limpie el output tras entrenar cada modelo del Hypertunning en el .search()\n",
    "import IPython\n",
    "import tensorflow as tf\n",
    "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
    "    def on_train_end(*args, **kwargs):\n",
    "        IPython.display.clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ejecución del proceso de tuning y al final ejecuta el callback\n",
    "tuner.search(\n",
    "    X_train, \n",
    "    Y_train,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val, Y_val),\n",
    "    callbacks=[ClearTrainingOutput()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtención de los hiperparámetros óptimos -> el best_hps es el modelo que ha encontrado como óptimo\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Units:\", best_hps.get('units'))\n",
    "print(\"Layers:\", best_hps.get('layers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    epochs = 20,\n",
    "    validation_data = (X_val, Y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.DataFrame(history.history)[['loss', 'val_loss']].plot(figsize=(10, 6))\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.DataFrame(history.history)[['accuracy', 'val_accuracy']].plot(figsize=(10, 6))\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
