{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificaci칩n de Tweets de desastres naturales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enunciado\n",
    "\n",
    "En esta competici칩n, se plantea la construcci칩n de un modelo de aprendizaje autom치tico que realice predicciones sobre qu칠 Tweets tratan de desastres reales y cu치les no.\n",
    "\n",
    "https://www.kaggle.com/vstepanenko/disaster-tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot') # gr치ficas con otro estilo m치s moderno\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lectura del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X_train = pd.read_csv('archive/train.csv') # los datos estan en un .csc\n",
    "X_test = pd.read_csv('archive/test.csv')\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualizaci칩n del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tama침o del conjunto de datos de entrenamiento: \", len(X_train))\n",
    "print(\"Tama침o del conjunto de datos de pruebas: \", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets de cada tipo que se encuentran en el conjunto de datos de entrenamiento\n",
    "X_train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['target'].hist()\n",
    "plt.ylabel(\"# tweets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siempre conveniente realizar un an치lisis exploratorio de la distribuci칩n de los datos para determinar la mejor manera de resolver el problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N칰mero de palabras por Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "# Calculamos el n칰mero de palabras\n",
    "tweet_len_0 = X_train[X_train['target'] == 0]['text'].str.split().map(lambda x: len(x))\n",
    "tweet_len_1 = X_train[X_train['target'] == 1]['text'].str.split().map(lambda x: len(x))\n",
    "ax1.hist(tweet_len_0, color='green')\n",
    "ax1.set_title('Non disaster tweets')\n",
    "ax2.hist(tweet_len_1, color='red')\n",
    "ax2.set_title('Disaster tweets')\n",
    "fig.suptitle('N칰mero de palabras por tweet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N칰mero de palabras 칰nicas por Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "# Calculamos el n칰mero de palabras\n",
    "tweet_len_0 = X_train[X_train['target'] == 0]['text'].str.split().map(lambda x: len(set(x)))\n",
    "tweet_len_1 = X_train[X_train['target'] == 1]['text'].str.split().map(lambda x: len(set(x)))\n",
    "ax1.hist(tweet_len_0, color='green')\n",
    "ax1.set_title('Non disaster tweets')\n",
    "ax2.hist(tweet_len_1, color='red')\n",
    "ax2.set_title('Disaster tweets')\n",
    "fig.suptitle('N칰mero de palabras por tweet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longitud media de las palabras por Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "# Calculamos el n칰mero de palabras\n",
    "tweet_len_0 = X_train[X_train['target'] == 0]['text'].str.split().map(lambda x: np.mean([len(i) for i in x]))\n",
    "tweet_len_1 = X_train[X_train['target'] == 1]['text'].str.split().map(lambda x: np.mean([len(i) for i in x]))\n",
    "ax1.hist(tweet_len_0, color='green')\n",
    "ax1.set_title('Non disaster tweets')\n",
    "ax2.hist(tweet_len_1, color='red')\n",
    "ax2.set_title('Disaster tweets')\n",
    "fig.suptitle('Longitud media de las palabras por Tweet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N칰mero de caracteres por tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "# Calculamos el n칰mero de caracteres por tweet\n",
    "tweet_len_0 = X_train[X_train['target'] == 0]['text'].str.len()\n",
    "tweet_len_1 = X_train[X_train['target'] == 1]['text'].str.len()\n",
    "ax1.hist(tweet_len_0, color='green')\n",
    "ax1.set_title('Non disaster tweets')\n",
    "ax2.hist(tweet_len_1, color='red')\n",
    "ax2.set_title('Disaster tweets')\n",
    "fig.suptitle('N칰mero caracteres por tweet')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podr칤amos seguir calculando caracter칤sticas de entrada como las siguientes:\n",
    "* N칰mero de palabras de fin por Tweet\n",
    "* N칰mero de urls por Tweet\n",
    "* Media de caracteres por Tweet\n",
    "* N칰mero de caracteres por Tweet\n",
    "* N칰mero de signos de puntuaci칩n por Tweet\n",
    "* N칰mero de hashtags por Tweet\n",
    "* N칰mero de @ por tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords m치s utilizadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas palabras no tienen un significado por si solas, sino que modifican o acompa침an a otras, este grupo suele estar conformado por art칤culos, pronombres, preposiciones, adverbios e incluso algunos verbos.\n",
    "\n",
    "En el procesamiento de datos en lenguaje natural son filtradas antes o despu칠s del proceso en si, no se consideran por su nulo significado, en el caso de los buscadores como Google no lo consideran al momento de posicionar, pero si al momento de mostrar los resultados de b칰squeda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stopwords(label):\n",
    "    tweets_stopwords = {}\n",
    "    for words in X_train[X_train['target'] == label]['text'].str.split():\n",
    "        sw = list(set(words).intersection(stopwords.words('english')))\n",
    "        for w in sw:\n",
    "            if w in tweets_stopwords.keys():\n",
    "                tweets_stopwords[w] += 1\n",
    "            else:\n",
    "                tweets_stopwords[w] = 1\n",
    "    top = sorted(tweets_stopwords.items(), key=lambda x:x[1],reverse=True)[:10]\n",
    "    plt.bar(*zip(*top))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stopwords(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stopwords(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An치lisis signos de puntuaci칩n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def plot_punctuation(label):\n",
    "    tweets_stopwords = {}\n",
    "    for words in X_train[X_train['target'] == label]['text'].str.split():\n",
    "        sw = list(set(words).intersection(string.punctuation))\n",
    "        for w in sw:\n",
    "            if w in tweets_stopwords.keys():\n",
    "                tweets_stopwords[w] += 1\n",
    "            else:\n",
    "                tweets_stopwords[w] = 1\n",
    "                \n",
    "    top = sorted(tweets_stopwords.items(), key=lambda x:x[1],reverse=True)[:20]\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(*zip(*top))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_punctuation(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_punctuation(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An치lisis de Ngramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(2, 2))\n",
    "sum_words = cv.fit_transform(X_train['text']).sum(axis=0)\n",
    "\n",
    "# Calculamos \n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\n",
    "words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.barh(*zip(*words_freq))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Limpieza del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_url(\"Esto es una prueba: http://localhost:8888/notebooks/Desktop/Workspace/Deep%20Neural%20Networks%20Course/11.%20Consideraciones%20de%20un%20proyecto%20de%20Deep%20Learning/code/Disaster%20Tweets.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class HTMLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs = True\n",
    "        self.fed = []\n",
    "        \n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "        \n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def remove_html(text):\n",
    "    s = HTMLStripper()\n",
    "    s.feed(text)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_html('<tr><td align=\"left\"><a href=\"../../issues/51/16.html#article\">Phrack World News</a></td>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_emoji(\"Omg another Earthquake 游땞游땞\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punctuation(\"hola #que tal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos nuestras funciones de limpieza del conjunto de datos\n",
    "X_train_prep = X_train.copy()\n",
    "\n",
    "X_train_prep['text'] = X_train['text'].apply(remove_url)\n",
    "X_train_prep['text'] = X_train['text'].apply(remove_html)\n",
    "X_train_prep['text'] = X_train['text'].apply(remove_emoji)\n",
    "X_train_prep['text'] = X_train['text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos nuestras funciones de limpieza del conjunto de datos\n",
    "X_test_prep = X_test.copy()\n",
    "\n",
    "X_test_prep['text'] = X_test['text'].apply(remove_url)\n",
    "X_test_prep['text'] = X_test['text'].apply(remove_html)\n",
    "X_test_prep['text'] = X_test['text'].apply(remove_emoji)\n",
    "X_test_prep['text'] = X_test['text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(2, 2))\n",
    "sum_words = cv.fit_transform(X_train_prep['text']).sum(axis=0)\n",
    "\n",
    "# Calculamos \n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\n",
    "words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.barh(*zip(*words_freq))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vectorizaci칩n del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = X_train_prep['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train_prep['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(X_test_prep['text'])\n",
    "X_test = X_test.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Divisi칩n del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Longitud subcojunto de entrenamiento: \", len(X_train))\n",
    "print(\"Longitud subconjunto de validaci칩n: \", len(X_val))\n",
    "print(\"Longitud subconjutno de pruebas: \", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Construcci칩n del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dropout(0.4))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dropout(0.4))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'Precision']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    epochs=20,\n",
    "    batch_size=1024,\n",
    "    validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history)[['loss', 'val_loss']].plot(figsize=(10, 7))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1.2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluaci칩n del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test).round(0)\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    print(\"{} - {}\".format(X_test_prep['text'][i], Y_pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
